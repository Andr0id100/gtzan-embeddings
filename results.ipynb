{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6e12f6f-8022-41f7-aed3-ea3dd2714b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d4fddca-5f02-473e-ab90-f941b16ec9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5b37738-4ab3-4258-a758-375170785843",
   "metadata": {},
   "outputs": [],
   "source": [
    "from embedding_api import EmbeddingAPI\n",
    "from classifier_utilty import create_classifier, train_classifier, test_classifier\n",
    "from data_utils import load_split\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c874699a-d7b8-41ea-8a49-08bb47af1398",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setup Data and API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cc09b91-dc5b-462f-bfaf-fd9ab0e68b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 599/599 [00:18<00:00, 32.41it/s]\n"
     ]
    }
   ],
   "source": [
    "train_audios, train_labels = load_split(\"splits/embedding_train_split.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "385d80ec-c542-49ad-b6c0-3f17522c1fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:05<00:00, 33.62it/s]\n"
     ]
    }
   ],
   "source": [
    "test_audios, test_labels = load_split(\"splits/classifier_test_split.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1753dc95-9880-4fc1-a4e2-451dbd3b066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "api = EmbeddingAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d3a5d12-11dc-4078-aa49-a021d6ab7b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(audios, api, model, embed_type=\"scene\", aggregation=\"mean\"):\n",
    "    embeddings = []\n",
    "    for audio in tqdm(audios):\n",
    "        if embed_type == \"scene\":\n",
    "            embedding = api.get_scene_embeddings(audio, model, aggregation=aggregation).numpy()\n",
    "        elif embed_type == \"timestamp\":\n",
    "            embedding, _ = api.get_timestamp_embeddings(audio, model)\n",
    "            embedding = embedding.numpy()\n",
    "            batch_size = embedding.shape[0]\n",
    "            embedding = embedding.reshape(batch_size, -1)\n",
    "        embeddings.extend(embedding)\n",
    "        \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9bf622-7d43-454d-89c3-e11580220730",
   "metadata": {},
   "source": [
    "# Training Info\n",
    "To pretrain the models (where needed), the GTZAN Genre Classification dataset was used. Since this is the same dataset which is supposed to be used for the evaluation of the linear classifier, a hold out split is created that is not used during the pretraining at all.  \n",
    "From the remaining dataset, a train and test split were created that were used during the training of the DL models. The same training split was used to train the linear classifier using the extracted embeddings as input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba068605-4b93-4e48-a2f3-0f9294e33498",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# MFCC Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1afcc8-1b46-4421-a59e-2b77843a78e5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Timestamp Embedding\n",
    "At each step, MFCC features are calculated and used as the embedding representation for that particular frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "161d7e83-3cd0-4644-99a6-57130bcf8d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load_model(\"saved_models/mfcc_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d6b4f33-2383-4277-bc19-3bdbbc1394b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 599/599 [00:05<00:00, 113.33it/s]\n"
     ]
    }
   ],
   "source": [
    "train_embeddings = get_embeddings(train_audios, api, model, embed_type=\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d11028af-f1f4-4bec-b0d8-e9402507c7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 107.08it/s]\n"
     ]
    }
   ],
   "source": [
    "test_embeddings = get_embeddings(test_audios, api, model, embed_type=\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49e0fc58-ed7d-40bd-8dd2-9bb1d5578da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.49\n"
     ]
    }
   ],
   "source": [
    "classifier = create_classifier()\n",
    "train_classifier(classifier, train_embeddings, train_labels)\n",
    "test_classifier(classifier, test_embeddings, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652ca677-67f5-4d1d-86af-2bf289f42a78",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Scene Embedding\n",
    "These are just the timestamp embeddings averaged over the time-steps to produce an aggregate representation of the entire sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51ef5f6b-6487-48f8-8578-385a2d24907a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 599/599 [00:04<00:00, 121.34it/s]\n"
     ]
    }
   ],
   "source": [
    "train_embeddings = get_embeddings(train_audios, api, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34b0b764-c81e-49da-8dfb-787b92c5db54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:01<00:00, 117.97it/s]\n"
     ]
    }
   ],
   "source": [
    "test_embeddings = get_embeddings(test_audios, api, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c6fe9c9-1aee-4046-b1ba-b61e8ddd9dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.585\n"
     ]
    }
   ],
   "source": [
    "classifier = create_classifier()\n",
    "train_classifier(classifier, train_embeddings, train_labels)\n",
    "test_classifier(classifier, test_embeddings, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54207d3-c07d-4573-9a49-d34f381598bb",
   "metadata": {},
   "source": [
    "The scene embeddings provide better results. This might be due to the fact that the amount of irrelevant information would be more in timestamp embeddings which have a much larger input (211264). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a165ae2-c4dd-40a4-98e1-0f3776945d31",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7591b1-ade6-470d-8c60-ce1cfa47051c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Timestamp Embeddings\n",
    "The MFCC features are provided as input to the LSTM model which produces a corresponding hidden state which in turn is used as the embedding representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72dc624f-352e-4642-ba9c-fc34d84a1025",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load_model(\"saved_models/lstm_model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff1b752f-f2d6-451a-8bd2-4a453f237e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 599/599 [00:08<00:00, 72.84it/s]\n"
     ]
    }
   ],
   "source": [
    "train_embeddings = get_embeddings(train_audios, api, model, embed_type=\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "347c43b1-66a6-46f6-a0ba-d5a354525792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:02<00:00, 69.00it/s]\n"
     ]
    }
   ],
   "source": [
    "test_embeddings = get_embeddings(test_audios, api, model, embed_type=\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37180ed6-8837-4a54-b31d-4ccd60a898f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "classifier = create_classifier()\n",
    "train_classifier(classifier, train_embeddings, train_labels)\n",
    "test_classifier(classifier, test_embeddings, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0103a59-e75f-45d1-8957-44bbab29b2fc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Scene Embeddings\n",
    "Since LSTMs aggregate information as they progress, the final hidden state is considered as the scene embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "323050e1-6e99-4d63-b132-5db4b2b694b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 599/599 [00:08<00:00, 73.28it/s]\n"
     ]
    }
   ],
   "source": [
    "train_embeddings = get_embeddings(train_audios, api, model, embed_type=\"scene\", aggregation=\"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "45af1bdc-9ed7-4df6-9976-646c1413099c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:02<00:00, 71.46it/s]\n"
     ]
    }
   ],
   "source": [
    "test_embeddings = get_embeddings(test_audios, api, model, embed_type=\"scene\", aggregation=\"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e370043e-f7fe-4832-8709-2eb033a0286a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.045\n"
     ]
    }
   ],
   "source": [
    "classifier = create_classifier()\n",
    "train_classifier(classifier, train_embeddings, train_labels)\n",
    "test_classifier(classifier, test_embeddings, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cfa6b9-890f-44ae-a37c-2cdccf94f9f4",
   "metadata": {},
   "source": [
    "Although the performance of the timestamp embeddings from the LSTM were comparable to MFCC, the scene embeddings produce atrocious results. This might be due to the fact that we are dealing with very large sequences (>3000 time-steps). This could result in the information content being diluted and the last state lacking any useful material."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb6e39f-b746-491a-9f33-d387938d73c6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Scene Embeddings (Mean Aggregation)\n",
    "Similar to how averaging over the time-steps provided decent results for the MFCC features, it is worth trying to collect the information from different LSTM hidden states by taking their mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd686fe7-8737-4510-a797-d7a5236465c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 599/599 [00:07<00:00, 78.01it/s]\n"
     ]
    }
   ],
   "source": [
    "train_embeddings = get_embeddings(train_audios, api, model, embed_type=\"scene\", aggregation=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0eeec022-6157-4775-8520-91ba2c3afa1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:02<00:00, 76.95it/s]\n"
     ]
    }
   ],
   "source": [
    "test_embeddings = get_embeddings(test_audios, api, model, embed_type=\"scene\", aggregation=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00dc6dcb-69e3-4807-b4d7-44cbd4a7fb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.275\n"
     ]
    }
   ],
   "source": [
    "classifier = create_classifier()\n",
    "train_classifier(classifier, train_embeddings, train_labels)\n",
    "test_classifier(classifier, test_embeddings, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff28166b-0c68-492f-9f1f-f70e8a0a2863",
   "metadata": {},
   "source": [
    "This provided us with somewhat decent results. \n",
    "\n",
    "One potential conclusion that we can draw from this is that information does not reach the final state. We can utilize this information to improve upon the pretraining part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf1d207-980b-4426-b76d-c259b52c8126",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LSTM Model (Mean Training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2985c898-0e9b-4bd8-a47c-c20192e767c5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Timestamp Embeddings\n",
    "Same as the previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90488985-27db-4409-84e7-eb2f42609543",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = api.load_model(\"saved_models/lstm_model_mean.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e8222ed-ca3e-485b-a79a-5f460e989fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 599/599 [00:07<00:00, 76.57it/s]\n"
     ]
    }
   ],
   "source": [
    "train_embeddings = get_embeddings(train_audios, api, model, embed_type=\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0277976-d29b-4956-9438-9c0a440001f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:02<00:00, 77.34it/s]\n"
     ]
    }
   ],
   "source": [
    "test_embeddings = get_embeddings(test_audios, api, model, embed_type=\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6b266bb-0a54-4f92-80f3-e25d132d7bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.565\n"
     ]
    }
   ],
   "source": [
    "classifier = create_classifier()\n",
    "train_classifier(classifier, train_embeddings, train_labels)\n",
    "test_classifier(classifier, test_embeddings, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a440a72-d8a6-4b4c-abf4-12ab19cdb2a9",
   "metadata": {},
   "source": [
    "This puts us much closer to the performance of the MFCC features. Using the mean of the hidden states allowed the DL model to not be constricted by the sequence and learn information that was more relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02c38eb-9b65-429b-bb89-e2e40bd6f023",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Scene Embeddings\n",
    "Averaging over the time-steps to produce a combined representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "922831af-4b1f-4b20-9965-0bf6cdec18cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 599/599 [00:07<00:00, 79.54it/s]\n"
     ]
    }
   ],
   "source": [
    "train_embeddings = get_embeddings(train_audios, api, model, embed_type=\"scene\", aggregation=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1ca41db-543a-4dd3-a06c-a826f23ee36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:02<00:00, 71.47it/s]\n"
     ]
    }
   ],
   "source": [
    "test_embeddings = get_embeddings(test_audios, api, model, embed_type=\"scene\", aggregation=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09fee2d5-6215-48eb-bafc-9206bd406295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.28\n"
     ]
    }
   ],
   "source": [
    "classifier = create_classifier()\n",
    "train_classifier(classifier, train_embeddings, train_labels)\n",
    "test_classifier(classifier, test_embeddings, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e66388b-ffb4-41d5-bc88-d05b0a14feed",
   "metadata": {},
   "source": [
    "Nothing significant compared to the scene embeddings of the previous model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e15c4a0-6ed7-4b69-8654-5757e424bd77",
   "metadata": {},
   "source": [
    "# Final Comments \n",
    "It is clear that there is some room for improvement in the process of aggregating information from the timestamp embeddings to create a more compact representation for the scene embeddings.  \n",
    "One potential avenue of exploration is to utilize attention mechanism which will provide a more sophisticated way to assemble the information into a single vector.  \n",
    "Additionaly, using different input for the pretraining task could allow the model to learn better. This could mean learning using Spectograms or the raw waveform directly but they have there own set of challenges associated with them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
